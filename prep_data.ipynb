{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a0a203-1518-4546-9c0d-dfe2e3126c30",
   "metadata": {},
   "source": [
    "## The Prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59a4887-5f78-48a2-8d47-1e49924c8bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"instruction\": \"You are an assistant that takes a piece of text that has been corrupted during OCR digitisation, and produce a corrected version of the same text.\",\n",
      "    \"input\": \"Hunterd pursue killer great white utalking Austrnlimn death bench (A JP) A JP - A great white shark that killed a teenago aurfer off tho South Auatralian capital Adohuide will be hunted down and destroyed after it returned to stalk tho city's beaches, authorities said.\",\n",
      "    \"output\": \"Hunters pursue killer great white stalking Australian death beach (AFP) AFP - A great white shark that killed a teenage surfer off the South Australian capital Adelaide will be hunted down and destroyed after it returned to stalk the city's beaches, authorities said.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!head -n1 ft_data/alpaca_data.jsonl | python -m json.tool --json-lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4dac6-7d00-4092-8f39-93ebbc7caefe",
   "metadata": {},
   "source": [
    "## The Config\n",
    "\n",
    "Pay close attention to `datasets` and `train_on_inputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e967700-792c-4d5c-b05e-20030b765550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model: meta-llama/Meta-Llama-3-8B\n",
      "model_type: AutoModelForCausalLM\n",
      "tokenizer_type: AutoTokenizer\n",
      "is_mistral_derived_model: true\n",
      "\n",
      "load_in_8bit: false\n",
      "load_in_4bit: true\n",
      "strict: false\n",
      "\n",
      "lora_fan_in_fan_out: false\n",
      "data_seed: 49\n",
      "seed: 49\n",
      "\n",
      "datasets:\n",
      "  - path: ft_data/alpaca_data.jsonl\n",
      "    type: alpaca\n",
      "dataset_prepared_path: last_run_prepared\n",
      "val_set_size: 0.1\n",
      "output_dir: ./qlora-alpaca-out\n",
      "hub_model_id: pbevan11/llama-3-8b-ocr-correction\n",
      "\n",
      "adapter: qlora\n",
      "lora_model_dir:\n",
      "\n",
      "sequence_len: 4096\n",
      "sample_packing: true\n",
      "pad_to_sequence_len: true\n",
      "\n",
      "lora_r: 32\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_linear: true\n",
      "lora_fan_in_fan_out:\n",
      "lora_target_modules:\n",
      "  - gate_proj\n",
      "  - down_proj\n",
      "  - up_proj\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - k_proj\n",
      "  - o_proj\n",
      "\n",
      "wandb_project: sncds/ocr-ft\n",
      "wandb_entity: peterbevan\n",
      "\n",
      "gradient_accumulation_steps: 4\n",
      "micro_batch_size: 2 # was 16\n",
      "eval_batch_size: 2 # was 16\n",
      "num_epochs: 4\n",
      "optimizer: paged_adamw_32bit\n",
      "lr_scheduler: cosine\n",
      "learning_rate: 0.0002\n",
      "\n",
      "train_on_inputs: false\n",
      "group_by_length: false\n",
      "bf16: auto\n",
      "fp16:\n",
      "tf32: false\n",
      "\n",
      "gradient_checkpointing: true\n",
      "early_stopping_patience:\n",
      "resume_from_checkpoint:\n",
      "local_rank:\n",
      "logging_steps: 1\n",
      "xformers_attention:\n",
      "flash_attention: true\n",
      "\n",
      "loss_watchdog_threshold: 5.0\n",
      "loss_watchdog_patience: 3\n",
      "\n",
      "warmup_steps: 10\n",
      "evals_per_epoch: 4\n",
      "eval_table_size:\n",
      "saves_per_epoch: 1\n",
      "debug:\n",
      "deepspeed:\n",
      "weight_decay: 0.0\n",
      "fsdp:\n",
      "fsdp_config:\n",
      "special_tokens:\n",
      "  pad_token: \"<|end_of_text|>\""
     ]
    }
   ],
   "source": [
    "!cat ax.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ec546-7ff5-4255-ab23-5596bc8e4ec3",
   "metadata": {},
   "source": [
    "## Do the Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb76432-78a2-4aca-a885-34e5fd735c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[2024-07-26 10:06:44,039] [INFO] [datasets.<module>:58] [PID:2312] PyTorch version 2.1.2+cu118 available.\n",
      "[2024-07-26 10:06:44,801] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-26 10:06:44,878] [INFO] [root.spawn:38] [PID:2312] gcc -pthread -B /root/miniconda3/envs/py3.10/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniconda3/envs/py3.10/include -fPIC -O2 -isystem /root/miniconda3/envs/py3.10/include -fPIC -c /tmp/tmpqoao59kv/test.c -o /tmp/tmpqoao59kv/test.o\n",
      "[2024-07-26 10:06:44,897] [INFO] [root.spawn:38] [PID:2312] gcc -pthread -B /root/miniconda3/envs/py3.10/compiler_compat /tmp/tmpqoao59kv/test.o -laio -o /tmp/tmpqoao59kv/a.out\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "                                 dP            dP   dP \n",
      "                                 88            88   88 \n",
      "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
      "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
      "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
      "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
      "                                                       \n",
      "                                                       \n",
      "\n",
      "****************************************\n",
      "**** Axolotl Dependency Versions *****\n",
      "  accelerate: 0.30.1         \n",
      "        peft: 0.11.1         \n",
      "transformers: 4.42.3         \n",
      "         trl: 0.8.7.dev0     \n",
      "       torch: 2.1.2+cu118    \n",
      "bitsandbytes: 0.43.1         \n",
      "****************************************\n",
      "[2024-07-26 10:06:46,466] [INFO] [axolotl.utils.config.models.input.check_eval_packing:918] [PID:2312] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
      "[2024-07-26 10:06:46,467] [DEBUG] [axolotl.normalize_config:80] [PID:2312] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-07-26 10:06:46,739] [INFO] [axolotl.normalize_config:183] [PID:2312] [RANK:0] GPU memory usage baseline: 0.000GB (+32.860GB misc)\u001b[39m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[2024-07-26 10:06:48,379] [DEBUG] [axolotl.load_tokenizer:280] [PID:2312] [RANK:0] EOS: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-07-26 10:06:48,379] [DEBUG] [axolotl.load_tokenizer:281] [PID:2312] [RANK:0] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-07-26 10:06:48,379] [DEBUG] [axolotl.load_tokenizer:282] [PID:2312] [RANK:0] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-07-26 10:06:48,379] [DEBUG] [axolotl.load_tokenizer:283] [PID:2312] [RANK:0] UNK: None / None\u001b[39m\n",
      "[2024-07-26 10:06:48,379] [INFO] [axolotl.load_tokenizer:294] [PID:2312] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-07-26 10:06:48,380] [INFO] [axolotl.load_tokenized_prepared_datasets:183] [PID:2312] [RANK:0] Unable to find prepared dataset in last_run_prepared/c62a9ba659f7db30b5f72d018bbd4d80\u001b[39m\n",
      "[2024-07-26 10:06:48,380] [INFO] [axolotl.load_tokenized_prepared_datasets:184] [PID:2312] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "[2024-07-26 10:06:49,456] [INFO] [axolotl.get_dataset_wrapper:540] [PID:2312] [RANK:0] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
      "[2024-07-26 10:06:49,919] [INFO] [axolotl.load_tokenized_prepared_datasets:414] [PID:2312] [RANK:0] merging datasets\u001b[39m\n",
      "[2024-07-26 10:06:49,932] [DEBUG] [axolotl.process_datasets_for_packing:188] [PID:2312] [RANK:0] min_input_len: 165\u001b[39m\n",
      "[2024-07-26 10:06:49,933] [DEBUG] [axolotl.process_datasets_for_packing:190] [PID:2312] [RANK:0] max_input_len: 499\u001b[39m\n",
      "Dropping Long Sequences (num_proc=64): 100%|█| 500/500 [00:00<00:00, 630.26 exam\n",
      "Add position_id column (Sample Packing) (num_proc=64): 100%|█| 500/500 [00:00<00\n",
      "[2024-07-26 10:06:53,649] [INFO] [axolotl.load_tokenized_prepared_datasets:427] [PID:2312] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/c62a9ba659f7db30b5f72d018bbd4d80\u001b[39m\n",
      "Saving the dataset (1/1 shards): 100%|█| 500/500 [00:00<00:00, 20870.71 examples\n",
      "[2024-07-26 10:06:53,696] [DEBUG] [axolotl.calculate_total_num_steps:299] [PID:2312] [RANK:0] total_num_tokens: 100_275\u001b[39m\n",
      "[2024-07-26 10:06:53,701] [DEBUG] [axolotl.calculate_total_num_steps:312] [PID:2312] [RANK:0] `total_supervised_tokens: 28_522`\u001b[39m\n",
      "[2024-07-26 10:06:59,400] [INFO] [axolotl.utils.samplers.multipack._len_est:185] [PID:2312] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 100275\u001b[39m\n",
      "[2024-07-26 10:06:59,400] [DEBUG] [axolotl.calculate_total_num_steps:364] [PID:2312] [RANK:0] data_loader_len: 2\u001b[39m\n",
      "[2024-07-26 10:06:59,400] [INFO] [axolotl.calc_sample_packing_eff_est:370] [PID:2312] [RANK:0] sample_packing_eff_est across ranks: [0.9415846604567307]\u001b[39m\n",
      "[2024-07-26 10:06:59,400] [DEBUG] [axolotl.calculate_total_num_steps:382] [PID:2312] [RANK:0] sample_packing_eff_est: 0.95\u001b[39m\n",
      "[2024-07-26 10:06:59,400] [DEBUG] [axolotl.calculate_total_num_steps:390] [PID:2312] [RANK:0] total_num_steps: 4\u001b[39m\n",
      "Loading checkpoint shards:   0%|                          | 0/4 [00:00<?, ?it/s]/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "Loading checkpoint shards:  25%|████▌             | 1/4 [00:00<00:01,  1.80it/s]/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "Loading checkpoint shards:  50%|█████████         | 2/4 [00:01<00:01,  1.75it/s]/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "Loading checkpoint shards:  75%|█████████████▌    | 3/4 [00:01<00:00,  1.74it/s]/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:01<00:00,  2.05it/s]\n",
      "[2024-07-26 10:07:02,000] [INFO] [axolotl.cli.preprocess.do_cli:81] [PID:2312] [RANK:0] \u001b[32mSuccess! Preprocessed data path: `dataset_prepared_path: last_run_prepared`\u001b[39m\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python -m axolotl.cli.preprocess llama3-ocr.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799030a-5b05-4559-a399-9811f7c28865",
   "metadata": {},
   "source": [
    "## Debug\n",
    "\n",
    "### See All The commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9841b32-3a27-4f60-8879-5d35acf8cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[2024-07-25 18:29:45,092] [INFO] [datasets.<module>:58] [PID:1706] PyTorch version 2.1.2+cu118 available.\n",
      "[2024-07-25 18:29:45,988] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-25 18:29:46,070] [INFO] [root.spawn:38] [PID:1706] gcc -pthread -B /root/miniconda3/envs/py3.10/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniconda3/envs/py3.10/include -fPIC -O2 -isystem /root/miniconda3/envs/py3.10/include -fPIC -c /tmp/tmppl35x243/test.c -o /tmp/tmppl35x243/test.o\n",
      "[2024-07-25 18:29:46,093] [INFO] [root.spawn:38] [PID:1706] gcc -pthread -B /root/miniconda3/envs/py3.10/compiler_compat /tmp/tmppl35x243/test.o -laio -o /tmp/tmppl35x243/a.out\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "                                 dP            dP   dP \n",
      "                                 88            88   88 \n",
      "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
      "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
      "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
      "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
      "                                                       \n",
      "                                                       \n",
      "\n",
      "****************************************\n",
      "**** Axolotl Dependency Versions *****\n",
      "  accelerate: 0.30.1         \n",
      "        peft: 0.11.1         \n",
      "transformers: 4.42.3         \n",
      "         trl: 0.8.7.dev0     \n",
      "       torch: 2.1.2+cu118    \n",
      "bitsandbytes: 0.43.1         \n",
      "****************************************\n",
      "[2024-07-25 18:29:47,860] [INFO] [axolotl.utils.config.models.input.check_eval_packing:908] [PID:1706] [RANK:0] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-07-25 18:29:47,861] [DEBUG] [axolotl.normalize_config:80] [PID:1706] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-07-25 18:29:48,133] [INFO] [axolotl.normalize_config:183] [PID:1706] [RANK:0] GPU memory usage baseline: 0.000GB (+0.467GB misc)\u001b[39m\n",
      "usage: preprocess.py [-h] [--debug [DEBUG]]\n",
      "                     [--debug_text_only [DEBUG_TEXT_ONLY]]\n",
      "                     [--debug_num_examples DEBUG_NUM_EXAMPLES]\n",
      "                     [--prompter PROMPTER] [--download [DOWNLOAD]]\n",
      "                     [--no_download]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --debug [DEBUG]\n",
      "  --debug_text_only [DEBUG_TEXT_ONLY]\n",
      "  --debug_num_examples DEBUG_NUM_EXAMPLES\n",
      "  --prompter PROMPTER\n",
      "  --download [DOWNLOAD]\n",
      "  --no_download\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python -m axolotl.cli.preprocess ax.yml --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f337b50-07e7-4a3e-bf76-bfb6b0fabe61",
   "metadata": {},
   "source": [
    "### Text Only\n",
    "\n",
    "I often have problems with `debug_text_only`, so I do things manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b82812f-76ce-4944-b330-de7ba383e6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x 5 root root  112 Jul 25 18:22 .\n",
      "drwxr-xr-x 8 root root 4.0K Jul 25 18:28 ..\n",
      "drwxr-xr-x 2 root root    6 Jul 15 21:48 .ipynb_checkpoints\n",
      "drwxr-xr-x 2 root root   82 Jul 25 18:22 7c12a0bc491bbea735059d4fd220fce3\n",
      "drwxr-xr-x 3 root root  108 Jul 15 22:15 f79576f2544449b45279c21970aa1125\n"
     ]
    }
   ],
   "source": [
    "!ls -lah last_run_prepared/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd3a555-be44-4814-bcae-6a45b2491e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import json, yaml\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "with open('ax.yml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "model_id = cfg['base_model']\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "ds = load_from_disk('last_run_prepared/f79576f2544449b45279c21970aa1125/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4e4fc-ac51-413a-89b7-bdd6f8bd83d5",
   "metadata": {},
   "source": [
    "Below is the assembled text in its flattened format.  Notice the spaces that axolotl are adding. Will talk about this at the end.\n",
    "\n",
    "This makes me paranoid because of differences between how the prompt is assembled and inference.  You just have to make sure its the same at inference!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f729546-5625-4f65-ada9-0c35a55509c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You are an assistant that takes a piece of text that has been corrupted during OCR digitisation, and produce a corrected version of the same text.\n",
      "\n",
      "### Input:\n",
      "WHO Urges Science t. Keep World Healthy (AP) AI'Technological advances that help drus companies churn out highly profitable prescription medications must also be applied to improving public health care arouud tb. giob., a World Health Orgammtion report released Wednesday says.\n",
      "\n",
      "### Response:\n",
      "WHO Urges Science to Keep World Healthy (AP) AP - Technological advances that help drug companies churn out highly profitable prescription medications must also be applied to improving public health care around the globe, a World Health Organization report released Wednesday says.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tok.decode(ds['input_ids'][111]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa59b1-8a95-4e65-b018-f259db75a75a",
   "metadata": {},
   "source": [
    "### Other Notes\n",
    "\n",
    "- Seeing the flattened version often helps you spot issues in your prompt.  It can be hard to notice that in jsonl format.\n",
    "- Check multiple examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8d205-e1b4-4d59-b24c-5fa78a473459",
   "metadata": {},
   "source": [
    "### Verbose debugging\n",
    "\n",
    "This helps you check things like: \n",
    "1. ignoring inputs (`train_on_inputs:False`) - notice the `red` color, which indicate tokens that are ignored.\n",
    "2. token ids (ex: what are those spaces right before `##`?\n",
    "3. The logs tell you what the special tokens are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c155f555-9660-46b2-b619-26ae226ad870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[2024-07-25 18:30:15,689] [INFO] [datasets.<module>:58] [PID:1763] PyTorch version 2.1.2+cu118 available.\n",
      "[2024-07-25 18:30:16,670] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-25 18:30:16,753] [INFO] [root.spawn:38] [PID:1763] gcc -pthread -B /root/miniconda3/envs/py3.10/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /root/miniconda3/envs/py3.10/include -fPIC -O2 -isystem /root/miniconda3/envs/py3.10/include -fPIC -c /tmp/tmpvlbqtv5c/test.c -o /tmp/tmpvlbqtv5c/test.o\n",
      "[2024-07-25 18:30:16,776] [INFO] [root.spawn:38] [PID:1763] gcc -pthread -B /root/miniconda3/envs/py3.10/compiler_compat /tmp/tmpvlbqtv5c/test.o -laio -o /tmp/tmpvlbqtv5c/a.out\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "                                 dP            dP   dP \n",
      "                                 88            88   88 \n",
      "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
      "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
      "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
      "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
      "                                                       \n",
      "                                                       \n",
      "\n",
      "****************************************\n",
      "**** Axolotl Dependency Versions *****\n",
      "  accelerate: 0.30.1         \n",
      "        peft: 0.11.1         \n",
      "transformers: 4.42.3         \n",
      "         trl: 0.8.7.dev0     \n",
      "       torch: 2.1.2+cu118    \n",
      "bitsandbytes: 0.43.1         \n",
      "****************************************\n",
      "[2024-07-25 18:30:18,508] [INFO] [axolotl.utils.config.models.input.check_eval_packing:908] [PID:1763] [RANK:0] explicitly setting `eval_sample_packing` to match `sample_packing`\u001b[39m\n",
      "[2024-07-25 18:30:18,509] [DEBUG] [axolotl.normalize_config:80] [PID:1763] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-07-25 18:30:19,180] [INFO] [axolotl.normalize_config:183] [PID:1763] [RANK:0] GPU memory usage baseline: 0.000GB (+0.467GB misc)\u001b[39m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[2024-07-25 18:30:20,928] [DEBUG] [axolotl.load_tokenizer:280] [PID:1763] [RANK:0] EOS: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-07-25 18:30:20,928] [DEBUG] [axolotl.load_tokenizer:281] [PID:1763] [RANK:0] BOS: 128000 / <|begin_of_text|>\u001b[39m\n",
      "[2024-07-25 18:30:20,928] [DEBUG] [axolotl.load_tokenizer:282] [PID:1763] [RANK:0] PAD: 128001 / <|end_of_text|>\u001b[39m\n",
      "[2024-07-25 18:30:20,928] [DEBUG] [axolotl.load_tokenizer:283] [PID:1763] [RANK:0] UNK: None / None\u001b[39m\n",
      "[2024-07-25 18:30:20,928] [INFO] [axolotl.load_tokenizer:294] [PID:1763] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-07-25 18:30:20,929] [INFO] [axolotl.load_tokenized_prepared_datasets:183] [PID:1763] [RANK:0] Unable to find prepared dataset in last_run_prepared/f79576f2544449b45279c21970aa1125\u001b[39m\n",
      "[2024-07-25 18:30:20,929] [INFO] [axolotl.load_tokenized_prepared_datasets:184] [PID:1763] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "[2024-07-25 18:30:22,020] [INFO] [axolotl.get_dataset_wrapper:540] [PID:1763] [RANK:0] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
      "[2024-07-25 18:30:22,538] [INFO] [axolotl.load_tokenized_prepared_datasets:414] [PID:1763] [RANK:0] merging datasets\u001b[39m\n",
      "[2024-07-25 18:30:22,558] [DEBUG] [axolotl.process_datasets_for_packing:188] [PID:1763] [RANK:0] min_input_len: 153\u001b[39m\n",
      "[2024-07-25 18:30:22,567] [DEBUG] [axolotl.process_datasets_for_packing:190] [PID:1763] [RANK:0] max_input_len: 718\u001b[39m\n",
      "[2024-07-25 18:30:22,567] [INFO] [axolotl.process_datasets_for_packing:195] [PID:1763] [RANK:0] dropping attention_mask column\u001b[39m\n",
      "Dropping Long Sequences (num_proc=64): 100%|█| 10000/10000 [00:00<00:00, 10703.8\n",
      "Add position_id column (Sample Packing) (num_proc=64): 100%|█| 10000/10000 [00:0\n",
      "[2024-07-25 18:30:27,830] [INFO] [axolotl.load_tokenized_prepared_datasets:427] [PID:1763] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/f79576f2544449b45279c21970aa1125\u001b[39m\n",
      "Saving the dataset (1/1 shards): 100%|█| 10000/10000 [00:00<00:00, 34944.06 exam\n",
      "[2024-07-25 18:30:28,155] [DEBUG] [axolotl.calculate_total_num_steps:299] [PID:1763] [RANK:0] total_num_tokens: 222_347\u001b[39m\n",
      "[2024-07-25 18:30:28,170] [DEBUG] [axolotl.calculate_total_num_steps:312] [PID:1763] [RANK:0] `total_supervised_tokens: 63_404`\u001b[39m\n",
      "[2024-07-25 18:30:34,272] [INFO] [axolotl.utils.samplers.multipack._len_est:185] [PID:1763] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 222347\u001b[39m\n",
      "[2024-07-25 18:30:34,272] [DEBUG] [axolotl.calculate_total_num_steps:364] [PID:1763] [RANK:0] data_loader_len: 6\u001b[39m\n",
      "[2024-07-25 18:30:34,272] [INFO] [axolotl.calc_sample_packing_eff_est:370] [PID:1763] [RANK:0] sample_packing_eff_est across ranks: [0.9693559919084821]\u001b[39m\n",
      "[2024-07-25 18:30:34,273] [DEBUG] [axolotl.calculate_total_num_steps:382] [PID:1763] [RANK:0] sample_packing_eff_est: None\u001b[39m\n",
      "[2024-07-25 18:30:34,273] [DEBUG] [axolotl.calculate_total_num_steps:390] [PID:1763] [RANK:0] total_num_steps: 24\u001b[39m\n",
      "[2024-07-25 18:30:34,285] [DEBUG] [axolotl.calculate_total_num_steps:299] [PID:1763] [RANK:0] total_num_tokens: 1_999_938\u001b[39m\n",
      "[2024-07-25 18:30:34,377] [DEBUG] [axolotl.calculate_total_num_steps:312] [PID:1763] [RANK:0] `total_supervised_tokens: 569_525`\u001b[39m\n",
      "[2024-07-25 18:30:34,403] [INFO] [axolotl.utils.samplers.multipack._len_est:185] [PID:1763] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 1999938\u001b[39m\n",
      "[2024-07-25 18:30:34,404] [DEBUG] [axolotl.calculate_total_num_steps:364] [PID:1763] [RANK:0] data_loader_len: 60\u001b[39m\n",
      "[2024-07-25 18:30:34,404] [INFO] [axolotl.calc_sample_packing_eff_est:370] [PID:1763] [RANK:0] sample_packing_eff_est across ranks: [0.9844074864541331]\u001b[39m\n",
      "[2024-07-25 18:30:34,404] [DEBUG] [axolotl.calculate_total_num_steps:382] [PID:1763] [RANK:0] sample_packing_eff_est: 0.99\u001b[39m\n",
      "[2024-07-25 18:30:34,404] [DEBUG] [axolotl.calculate_total_num_steps:390] [PID:1763] [RANK:0] total_num_steps: 240\u001b[39m\n",
      "[2024-07-25 18:30:34,404] [INFO] [axolotl.scripts.load_datasets:408] [PID:1763] [RANK:0] check_dataset_labels...\u001b[39m\n",
      "[2024-07-25 18:30:34,414] [INFO] [axolotl.check_example_labels:45] [PID:1763] [RANK:0] \u001b[31m<|begin_of_text|>\u001b[0m\u001b[97m(-100, 128000)\u001b[0m \u001b[31mBelow\u001b[0m\u001b[97m(-100, 39314)\u001b[0m \u001b[31m is\u001b[0m\u001b[97m(-100, 374)\u001b[0m \u001b[31m an\u001b[0m\u001b[97m(-100, 459)\u001b[0m \u001b[31m instruction\u001b[0m\u001b[97m(-100, 7754)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 430)\u001b[0m \u001b[31m describes\u001b[0m\u001b[97m(-100, 16964)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m task\u001b[0m\u001b[97m(-100, 3465)\u001b[0m \u001b[31m,\u001b[0m\u001b[97m(-100, 11)\u001b[0m \u001b[31m paired\u001b[0m\u001b[97m(-100, 35526)\u001b[0m \u001b[31m with\u001b[0m\u001b[97m(-100, 449)\u001b[0m \u001b[31m an\u001b[0m\u001b[97m(-100, 459)\u001b[0m \u001b[31m input\u001b[0m\u001b[97m(-100, 1988)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 430)\u001b[0m \u001b[31m provides\u001b[0m\u001b[97m(-100, 5825)\u001b[0m \u001b[31m further\u001b[0m\u001b[97m(-100, 4726)\u001b[0m \u001b[31m context\u001b[0m\u001b[97m(-100, 2317)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 13)\u001b[0m \u001b[31m Write\u001b[0m\u001b[97m(-100, 9842)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m response\u001b[0m\u001b[97m(-100, 2077)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 430)\u001b[0m \u001b[31m appropriately\u001b[0m\u001b[97m(-100, 36001)\u001b[0m \u001b[31m completes\u001b[0m\u001b[97m(-100, 45695)\u001b[0m \u001b[31m the\u001b[0m\u001b[97m(-100, 279)\u001b[0m \u001b[31m request\u001b[0m\u001b[97m(-100, 1715)\u001b[0m \u001b[31m.\n",
      "\n",
      "\u001b[0m\u001b[97m(-100, 382)\u001b[0m \u001b[31m###\u001b[0m\u001b[97m(-100, 14711)\u001b[0m \u001b[31m Instruction\u001b[0m\u001b[97m(-100, 30151)\u001b[0m \u001b[31m:\n",
      "\u001b[0m\u001b[97m(-100, 512)\u001b[0m \u001b[31mYou\u001b[0m\u001b[97m(-100, 2675)\u001b[0m \u001b[31m are\u001b[0m\u001b[97m(-100, 527)\u001b[0m \u001b[31m an\u001b[0m\u001b[97m(-100, 459)\u001b[0m \u001b[31m assistant\u001b[0m\u001b[97m(-100, 18328)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 430)\u001b[0m \u001b[31m takes\u001b[0m\u001b[97m(-100, 5097)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m piece\u001b[0m\u001b[97m(-100, 6710)\u001b[0m \u001b[31m of\u001b[0m\u001b[97m(-100, 315)\u001b[0m \u001b[31m text\u001b[0m\u001b[97m(-100, 1495)\u001b[0m \u001b[31m that\u001b[0m\u001b[97m(-100, 430)\u001b[0m \u001b[31m has\u001b[0m\u001b[97m(-100, 706)\u001b[0m \u001b[31m been\u001b[0m\u001b[97m(-100, 1027)\u001b[0m \u001b[31m corrupted\u001b[0m\u001b[97m(-100, 50500)\u001b[0m \u001b[31m during\u001b[0m\u001b[97m(-100, 2391)\u001b[0m \u001b[31m OCR\u001b[0m\u001b[97m(-100, 81677)\u001b[0m \u001b[31m digit\u001b[0m\u001b[97m(-100, 16099)\u001b[0m \u001b[31misation\u001b[0m\u001b[97m(-100, 8082)\u001b[0m \u001b[31m,\u001b[0m\u001b[97m(-100, 11)\u001b[0m \u001b[31m and\u001b[0m\u001b[97m(-100, 323)\u001b[0m \u001b[31m produce\u001b[0m\u001b[97m(-100, 8356)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m corrected\u001b[0m\u001b[97m(-100, 37065)\u001b[0m \u001b[31m version\u001b[0m\u001b[97m(-100, 2373)\u001b[0m \u001b[31m of\u001b[0m\u001b[97m(-100, 315)\u001b[0m \u001b[31m the\u001b[0m\u001b[97m(-100, 279)\u001b[0m \u001b[31m same\u001b[0m\u001b[97m(-100, 1890)\u001b[0m \u001b[31m text\u001b[0m\u001b[97m(-100, 1495)\u001b[0m \u001b[31m.\n",
      "\n",
      "\u001b[0m\u001b[97m(-100, 382)\u001b[0m \u001b[31m###\u001b[0m\u001b[97m(-100, 14711)\u001b[0m \u001b[31m Input\u001b[0m\u001b[97m(-100, 5688)\u001b[0m \u001b[31m:\n",
      "\u001b[0m\u001b[97m(-100, 512)\u001b[0m \u001b[31mBrazil\u001b[0m\u001b[97m(-100, 68299)\u001b[0m \u001b[31m I\u001b[0m\u001b[97m(-100, 358)\u001b[0m \u001b[31m.i\u001b[0m\u001b[97m(-100, 8777)\u001b[0m \u001b[31mui\u001b[0m\u001b[97m(-100, 2005)\u001b[0m \u001b[31mi\u001b[0m\u001b[97m(-100, 72)\u001b[0m \u001b[31mil\u001b[0m\u001b[97m(-100, 321)\u001b[0m \u001b[31mc\u001b[0m\u001b[97m(-100, 66)\u001b[0m \u001b[31m ring\u001b[0m\u001b[97m(-100, 10264)\u001b[0m \u001b[31m I\u001b[0m\u001b[97m(-100, 358)\u001b[0m \u001b[31m'\u001b[0m\u001b[97m(-100, 6)\u001b[0m \u001b[31mro\u001b[0m\u001b[97m(-100, 299)\u001b[0m \u001b[31mlio\u001b[0m\u001b[97m(-100, 26205)\u001b[0m \u001b[31m.\u001b[0m\u001b[97m(-100, 662)\u001b[0m \u001b[31mSt\u001b[0m\u001b[97m(-100, 626)\u001b[0m \u001b[31mlng\u001b[0m\u001b[97m(-100, 21476)\u001b[0m \u001b[31mK\u001b[0m\u001b[97m(-100, 42)\u001b[0m \u001b[31m It\u001b[0m\u001b[97m(-100, 1102)\u001b[0m \u001b[31mauf\u001b[0m\u001b[97m(-100, 51628)\u001b[0m \u001b[31mcors\u001b[0m\u001b[97m(-100, 55072)\u001b[0m \u001b[31m,\u001b[0m\u001b[97m(-100, 1174)\u001b[0m \u001b[31m Polit\u001b[0m\u001b[97m(-100, 16307)\u001b[0m \u001b[31mician\u001b[0m\u001b[97m(-100, 12734)\u001b[0m \u001b[31m*\u001b[0m\u001b[97m(-100, 9)\u001b[0m \u001b[31m I\u001b[0m\u001b[97m(-100, 358)\u001b[0m \u001b[31m \u001b[0m\u001b[97m(-100, 220)\u001b[0m \u001b[31m BRA\u001b[0m\u001b[97m(-100, 74863)\u001b[0m \u001b[31m8\u001b[0m\u001b[97m(-100, 23)\u001b[0m \u001b[31mIL\u001b[0m\u001b[97m(-100, 1750)\u001b[0m \u001b[31mIA\u001b[0m\u001b[97m(-100, 5987)\u001b[0m \u001b[31m,\u001b[0m\u001b[97m(-100, 1174)\u001b[0m \u001b[31m Brazil\u001b[0m\u001b[97m(-100, 16327)\u001b[0m \u001b[31m (\u001b[0m\u001b[97m(-100, 320)\u001b[0m \u001b[31m I\u001b[0m\u001b[97m(-100, 358)\u001b[0m \u001b[31mtri\u001b[0m\u001b[97m(-100, 23254)\u001b[0m \u001b[31mit\u001b[0m\u001b[97m(-100, 275)\u001b[0m \u001b[31mcr\u001b[0m\u001b[97m(-100, 5192)\u001b[0m \u001b[31mH\u001b[0m\u001b[97m(-100, 39)\u001b[0m \u001b[31m )\u001b[0m\u001b[97m(-100, 883)\u001b[0m \u001b[31m _\u001b[0m\u001b[97m(-100, 721)\u001b[0m \u001b[31m A\u001b[0m\u001b[97m(-100, 362)\u001b[0m \u001b[31m Brazilian\u001b[0m\u001b[97m(-100, 36083)\u001b[0m \u001b[31m cn\u001b[0m\u001b[97m(-100, 13979)\u001b[0m \u001b[31mii\u001b[0m\u001b[97m(-100, 3893)\u001b[0m \u001b[31mK\u001b[0m\u001b[97m(-100, 42)\u001b[0m \u001b[31mrc\u001b[0m\u001b[97m(-100, 1310)\u001b[0m \u001b[31mH\u001b[0m\u001b[97m(-100, 39)\u001b[0m \u001b[31msl\u001b[0m\u001b[97m(-100, 3306)\u001b[0m \u001b[31mon\u001b[0m\u001b[97m(-100, 263)\u001b[0m \u001b[31mul\u001b[0m\u001b[97m(-100, 360)\u001b[0m \u001b[31m \u001b[0m\u001b[97m(-100, 220)\u001b[0m \u001b[31m i\u001b[0m\u001b[97m(-100, 602)\u001b[0m \u001b[31mro\u001b[0m\u001b[97m(-100, 299)\u001b[0m \u001b[31mbo\u001b[0m\u001b[97m(-100, 754)\u001b[0m \u001b[31m ac\u001b[0m\u001b[97m(-100, 1645)\u001b[0m \u001b[31mf\u001b[0m\u001b[97m(-100, 69)\u001b[0m \u001b[31mii\u001b[0m\u001b[97m(-100, 3893)\u001b[0m \u001b[31mHi\u001b[0m\u001b[97m(-100, 13347)\u001b[0m \u001b[31m-d\u001b[0m\u001b[97m(-100, 1773)\u001b[0m \u001b[31m \u001b[0m\u001b[97m(-100, 220)\u001b[0m \u001b[31m91\u001b[0m\u001b[97m(-100, 5925)\u001b[0m \u001b[31m p\u001b[0m\u001b[97m(-100, 281)\u001b[0m \u001b[31mcon\u001b[0m\u001b[97m(-100, 444)\u001b[0m \u001b[31mlo\u001b[0m\u001b[97m(-100, 385)\u001b[0m \u001b[31m,\u001b[0m\u001b[97m(-100, 1174)\u001b[0m \u001b[31m Ini\u001b[0m\u001b[97m(-100, 74867)\u001b[0m \u001b[31m'\u001b[0m\u001b[97m(-100, 6)\u001b[0m \u001b[31mli\u001b[0m\u001b[97m(-100, 747)\u001b[0m \u001b[31mid\u001b[0m\u001b[97m(-100, 307)\u001b[0m \u001b[31miii\u001b[0m\u001b[97m(-100, 35694)\u001b[0m \u001b[31mK\u001b[0m\u001b[97m(-100, 42)\u001b[0m \u001b[31m a\u001b[0m\u001b[97m(-100, 264)\u001b[0m \u001b[31m form\u001b[0m\u001b[97m(-100, 1376)\u001b[0m \u001b[31m or\u001b[0m\u001b[97m(-100, 477)\u001b[0m \u001b[31m contr\u001b[0m\u001b[97m(-100, 6155)\u001b[0m \u001b[31mail\u001b[0m\u001b[97m(-100, 607)\u001b[0m \u001b[31m h\u001b[0m\u001b[97m(-100, 305)\u001b[0m \u001b[31munk\u001b[0m\u001b[97m(-100, 3200)\u001b[0m \u001b[31m li\u001b[0m\u001b[97m(-100, 908)\u001b[0m \u001b[31mio\u001b[0m\u001b[97m(-100, 822)\u001b[0m \u001b[31mf\u001b[0m\u001b[97m(-100, 69)\u001b[0m \u001b[31m \u001b[0m\u001b[97m(-100, 220)\u001b[0m \u001b[31m iii\u001b[0m\u001b[97m(-100, 63193)\u001b[0m \u001b[31mul\u001b[0m\u001b[97m(-100, 360)\u001b[0m \u001b[31m nn\u001b[0m\u001b[97m(-100, 11120)\u001b[0m \u001b[31m ox\u001b[0m\u001b[97m(-100, 19488)\u001b[0m \u001b[31m ni\u001b[0m\u001b[97m(-100, 13080)\u001b[0m \u001b[31mu\u001b[0m\u001b[97m(-100, 84)\u001b[0m \u001b[31m vor\u001b[0m\u001b[97m(-100, 14230)\u001b[0m \u001b[31m of\u001b[0m\u001b[97m(-100, 315)\u001b[0m \u001b[31m Sil\u001b[0m\u001b[97m(-100, 8211)\u001b[0m \u001b[31mo\u001b[0m\u001b[97m(-100, 78)\u001b[0m \u001b[31m I\u001b[0m\u001b[97m(-100, 358)\u001b[0m \u001b[31m'n\u001b[0m\u001b[97m(-100, 44886)\u001b[0m \u001b[31mulo\u001b[0m\u001b[97m(-100, 8938)\u001b[0m \u001b[31m,\u001b[0m\u001b[97m(-100, 1174)\u001b[0m \u001b[31m on\u001b[0m\u001b[97m(-100, 389)\u001b[0m \u001b[31m T\u001b[0m\u001b[97m(-100, 350)\u001b[0m \u001b[31mii\u001b[0m\u001b[97m(-100, 3893)\u001b[0m \u001b[31mow\u001b[0m\u001b[97m(-100, 363)\u001b[0m \u001b[31mIn\u001b[0m\u001b[97m(-100, 644)\u001b[0m \u001b[31my\u001b[0m\u001b[97m(-100, 88)\u001b[0m \u001b[31m of\u001b[0m\u001b[97m(-100, 315)\u001b[0m \u001b[31m invol\u001b[0m\u001b[97m(-100, 4457)\u001b[0m \u001b[31mv\u001b[0m\u001b[97m(-100, 85)\u001b[0m \u001b[31moin\u001b[0m\u001b[97m(-100, 2003)\u001b[0m \u001b[31mont\u001b[0m\u001b[97m(-100, 546)\u001b[0m \u001b[31m in\u001b[0m\u001b[97m(-100, 304)\u001b[0m \u001b[31m \u001b[0m\u001b[97m(-100, 220)\u001b[0m \u001b[31m Hi\u001b[0m\u001b[97m(-100, 21694)\u001b[0m \u001b[31mis\u001b[0m\u001b[97m(-100, 285)\u001b[0m \u001b[31mno\u001b[0m\u001b[97m(-100, 2201)\u001b[0m \u001b[31mct\u001b[0m\u001b[97m(-100, 302)\u001b[0m \u001b[31mod\u001b[0m\u001b[97m(-100, 347)\u001b[0m \u001b[31m li\u001b[0m\u001b[97m(-100, 908)\u001b[0m \u001b[31mii\u001b[0m\u001b[97m(-100, 3893)\u001b[0m \u001b[31m nd\u001b[0m\u001b[97m(-100, 15953)\u001b[0m \u001b[31mnr\u001b[0m\u001b[97m(-100, 20191)\u001b[0m \u001b[31min\u001b[0m\u001b[97m(-100, 258)\u001b[0m \u001b[31mjf\u001b[0m\u001b[97m(-100, 97523)\u001b[0m \u001b[31m or\u001b[0m\u001b[97m(-100, 477)\u001b[0m \u001b[31m mon\u001b[0m\u001b[97m(-100, 1647)\u001b[0m \u001b[31moy\u001b[0m\u001b[97m(-100, 2303)\u001b[0m \u001b[31m r\u001b[0m\u001b[97m(-100, 436)\u001b[0m \u001b[31mrom\u001b[0m\u001b[97m(-100, 442)\u001b[0m \u001b[31m dni\u001b[0m\u001b[97m(-100, 61859)\u001b[0m \u001b[31mK\u001b[0m\u001b[97m(-100, 42)\u001b[0m \u001b[31m tr\u001b[0m\u001b[97m(-100, 490)\u001b[0m \u001b[31mal\u001b[0m\u001b[97m(-100, 278)\u001b[0m \u001b[31mU\u001b[0m\u001b[97m(-100, 52)\u001b[0m \u001b[31mck\u001b[0m\u001b[97m(-100, 377)\u001b[0m \u001b[31min\u001b[0m\u001b[97m(-100, 258)\u001b[0m \u001b[31mir\u001b[0m\u001b[97m(-100, 404)\u001b[0m \u001b[31m,\u001b[0m\u001b[97m(-100, 1174)\u001b[0m \u001b[31m H\u001b[0m\u001b[97m(-100, 473)\u001b[0m \u001b[31mmn\u001b[0m\u001b[97m(-100, 22524)\u001b[0m \u001b[31mirm\u001b[0m\u001b[97m(-100, 2923)\u001b[0m \u001b[31mll\u001b[0m\u001b[97m(-100, 657)\u001b[0m \u001b[31mna\u001b[0m\u001b[97m(-100, 3458)\u001b[0m \u001b[31m'\u001b[0m\u001b[97m(-100, 6)\u001b[0m \u001b[31m \u001b[0m\u001b[97m(-100, 220)\u001b[0m \u001b[31m unit\u001b[0m\u001b[97m(-100, 5089)\u001b[0m \u001b[31m o\u001b[0m\u001b[97m(-100, 297)\u001b[0m \u001b[31mll\u001b[0m\u001b[97m(-100, 657)\u001b[0m \u001b[31mior\u001b[0m\u001b[97m(-100, 2521)\u001b[0m \u001b[31m cr\u001b[0m\u001b[97m(-100, 1589)\u001b[0m \u001b[31min\u001b[0m\u001b[97m(-100, 258)\u001b[0m \u001b[31mio\u001b[0m\u001b[97m(-100, 822)\u001b[0m \u001b[31mH\u001b[0m\u001b[97m(-100, 39)\u001b[0m \u001b[31m.\n",
      "\n",
      "\u001b[0m\u001b[97m(-100, 382)\u001b[0m \u001b[31m###\u001b[0m\u001b[97m(-100, 14711)\u001b[0m \u001b[31m Response\u001b[0m\u001b[97m(-100, 6075)\u001b[0m \u001b[31m:\n",
      "\u001b[0m\u001b[97m(-100, 512)\u001b[0m \u001b[32mBrazil\u001b[0m\u001b[97m(68299, 68299)\u001b[0m \u001b[32m Lau\u001b[0m\u001b[97m(86091, 86091)\u001b[0m \u001b[32mnder\u001b[0m\u001b[97m(910, 910)\u001b[0m \u001b[32ming\u001b[0m\u001b[97m(287, 287)\u001b[0m \u001b[32m Probe\u001b[0m\u001b[97m(78378, 78378)\u001b[0m \u001b[32m St\u001b[0m\u001b[97m(800, 800)\u001b[0m \u001b[32mings\u001b[0m\u001b[97m(826, 826)\u001b[0m \u001b[32m Bank\u001b[0m\u001b[97m(8715, 8715)\u001b[0m \u001b[32mers\u001b[0m\u001b[97m(388, 388)\u001b[0m \u001b[32m,\u001b[0m\u001b[97m(11, 11)\u001b[0m \u001b[32m Polit\u001b[0m\u001b[97m(16307, 16307)\u001b[0m \u001b[32micians\u001b[0m\u001b[97m(9818, 9818)\u001b[0m \u001b[32m \u001b[0m\u001b[97m(220, 220)\u001b[0m \u001b[32m BR\u001b[0m\u001b[97m(19333, 19333)\u001b[0m \u001b[32mAS\u001b[0m\u001b[97m(1950, 1950)\u001b[0m \u001b[32mIL\u001b[0m\u001b[97m(1750, 1750)\u001b[0m \u001b[32mIA\u001b[0m\u001b[97m(5987, 5987)\u001b[0m \u001b[32m,\u001b[0m\u001b[97m(11, 11)\u001b[0m \u001b[32m Brazil\u001b[0m\u001b[97m(16327, 16327)\u001b[0m \u001b[32m (\u001b[0m\u001b[97m(320, 320)\u001b[0m \u001b[32mReuters\u001b[0m\u001b[97m(36978, 36978)\u001b[0m \u001b[32m)\u001b[0m\u001b[97m(8, 8)\u001b[0m \u001b[32m -\u001b[0m\u001b[97m(482, 482)\u001b[0m \u001b[32m A\u001b[0m\u001b[97m(362, 362)\u001b[0m \u001b[32m Brazilian\u001b[0m\u001b[97m(36083, 36083)\u001b[0m \u001b[32m congressional\u001b[0m\u001b[97m(31719, 31719)\u001b[0m \u001b[32m \u001b[0m\u001b[97m(220, 220)\u001b[0m \u001b[32m probe\u001b[0m\u001b[97m(22477, 22477)\u001b[0m \u001b[32m accused\u001b[0m\u001b[97m(13487, 13487)\u001b[0m \u001b[32m \u001b[0m\u001b[97m(220, 220)\u001b[0m \u001b[32m91\u001b[0m\u001b[97m(5925, 5925)\u001b[0m \u001b[32m people\u001b[0m\u001b[97m(1274, 1274)\u001b[0m \u001b[32m,\u001b[0m\u001b[97m(11, 11)\u001b[0m \u001b[32m including\u001b[0m\u001b[97m(2737, 2737)\u001b[0m \u001b[32m a\u001b[0m\u001b[97m(264, 264)\u001b[0m \u001b[32m former\u001b[0m\u001b[97m(4846, 4846)\u001b[0m \u001b[32m central\u001b[0m\u001b[97m(8792, 8792)\u001b[0m \u001b[32m bank\u001b[0m\u001b[97m(6201, 6201)\u001b[0m \u001b[32m chief\u001b[0m\u001b[97m(10388, 10388)\u001b[0m \u001b[32m \u001b[0m\u001b[97m(220, 220)\u001b[0m \u001b[32m and\u001b[0m\u001b[97m(323, 323)\u001b[0m \u001b[32m an\u001b[0m\u001b[97m(459, 459)\u001b[0m \u001b[32m ex\u001b[0m\u001b[97m(506, 506)\u001b[0m \u001b[32m-m\u001b[0m\u001b[97m(1474, 1474)\u001b[0m \u001b[32may\u001b[0m\u001b[97m(352, 352)\u001b[0m \u001b[32mor\u001b[0m\u001b[97m(269, 269)\u001b[0m \u001b[32m of\u001b[0m\u001b[97m(315, 315)\u001b[0m \u001b[32m Sao\u001b[0m\u001b[97m(90972, 90972)\u001b[0m \u001b[32m Paulo\u001b[0m\u001b[97m(38026, 38026)\u001b[0m \u001b[32m,\u001b[0m\u001b[97m(11, 11)\u001b[0m \u001b[32m on\u001b[0m\u001b[97m(389, 389)\u001b[0m \u001b[32m Tuesday\u001b[0m\u001b[97m(7742, 7742)\u001b[0m \u001b[32m of\u001b[0m\u001b[97m(315, 315)\u001b[0m \u001b[32m involvement\u001b[0m\u001b[97m(22315, 22315)\u001b[0m \u001b[32m in\u001b[0m\u001b[97m(304, 304)\u001b[0m \u001b[32m \u001b[0m\u001b[97m(220, 220)\u001b[0m \u001b[32m suspected\u001b[0m\u001b[97m(24740, 24740)\u001b[0m \u001b[32m laundering\u001b[0m\u001b[97m(64402, 64402)\u001b[0m \u001b[32m of\u001b[0m\u001b[97m(315, 315)\u001b[0m \u001b[32m money\u001b[0m\u001b[97m(3300, 3300)\u001b[0m \u001b[32m from\u001b[0m\u001b[97m(505, 505)\u001b[0m \u001b[32m drug\u001b[0m\u001b[97m(5623, 5623)\u001b[0m \u001b[32m trafficking\u001b[0m\u001b[97m(34563, 34563)\u001b[0m \u001b[32m,\u001b[0m\u001b[97m(11, 11)\u001b[0m \u001b[32m smuggling\u001b[0m\u001b[97m(74034, 74034)\u001b[0m \u001b[32m \u001b[0m\u001b[97m(220, 220)\u001b[0m \u001b[32m and\u001b[0m\u001b[97m(323, 323)\u001b[0m \u001b[32m other\u001b[0m\u001b[97m(1023, 1023)\u001b[0m \u001b[32m crimes\u001b[0m\u001b[97m(17073, 17073)\u001b[0m \u001b[32m.\u001b[0m\u001b[97m(13, 13)\u001b[0m \u001b[32m<|end_of_text|>\u001b[0m\u001b[97m(128001, 128001)\u001b[0m\u001b[39m\n",
      "[2024-07-25 18:30:34,414] [INFO] [axolotl.check_example_labels:46] [PID:1763] [RANK:0] \n",
      "\n",
      "\n",
      "\u001b[39m\n",
      "[2024-07-25 18:30:34,414] [INFO] [axolotl.scripts.load_datasets:421] [PID:1763] [RANK:0] printing prompters...\u001b[39m\n",
      "[2024-07-25 18:30:34,414] [INFO] [axolotl.scripts.load_datasets:423] [PID:1763] [RANK:0] \n",
      "<start>\n",
      "\u001b[36mBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "{instruction}\n",
      "\n",
      "### Input:\n",
      "{input}\n",
      "\n",
      "### Response:\n",
      "{output}\u001b[39m\n",
      "<end>\n",
      "\u001b[39m\n",
      "Loading checkpoint shards:   0%|                          | 0/4 [00:00<?, ?it/s]/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "Loading checkpoint shards:  25%|████▌             | 1/4 [00:00<00:00,  3.03it/s]/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "Loading checkpoint shards:  50%|█████████         | 2/4 [00:00<00:00,  3.51it/s]/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "Loading checkpoint shards:  75%|█████████████▌    | 3/4 [00:00<00:00,  3.74it/s]/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:01<00:00,  3.77it/s]\n",
      "[2024-07-25 18:30:36,092] [INFO] [axolotl.cli.preprocess.do_cli:81] [PID:1763] [RANK:0] \u001b[32mSuccess! Preprocessed data path: `dataset_prepared_path: last_run_prepared`\u001b[39m\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python -m axolotl.cli.preprocess ax.yml --debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab233f-fb90-4226-b0c7-f65031155c4c",
   "metadata": {},
   "source": [
    "## Look at special tokens\n",
    "\n",
    "Ex: What is `<0x0A>`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1025600-a2fc-4a7e-8a0e-66aa7cdc0691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode([13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e6ca7-adf9-4b1d-849e-6b3267371841",
   "metadata": {},
   "source": [
    "**But where is the space coming from?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a4bd50-0dd4-47f7-a47c-873f21b18f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eth'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(774)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbd9c6-05c0-41b6-bac0-d0acf4667b84",
   "metadata": {},
   "source": [
    "**It's pretty confusing!  See [this blog post](https://hamel.dev/notes/llm/finetuning/05_tokenizer_gotchas.html)**\n",
    "\n",
    "What does Wing think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781e472-e120-4f9b-867b-9adc602d7135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
